\section{Introduction}
In recent years, the rise of text-to-image models has changed the way we approach the generation of images, allowing anyone to generate high-quality images from nothing more than text prompts. In this area, Stable Diffusion~\cite{rombach2022stablediffusion} has been a milestone model that advanced the field in terms of efficiency and quality in image generation. However, with the recent growing concerns over the computational cost, energy consumption, and control of the output, recent works such as Würstchen and ControlNet have come up with novel approaches to meet those challenges while maintaining or enhancing image quality. In this paper, we investigate Stable Diffusion, Würstchen~\cite{pernias2024wrstchen}, and ControlNet~\cite{zhang2023addingconditionalcontroltexttoimage} by discussing the architecture of each, re-implementing an experiment from their respective papers, and discuss their results. We also debate the greater social consequences of such technologies, particularly in light of developments in text-to-image AI.
\\
\\
\noindent
\textbf{Stable Diffusion} achieved its improvements in efficiency and quality by not generating images directly in the pixel space, which is computationally expensive. Instead, it performs its calculations in latent space, a more abstract representation of image data in which synthesis complexity is drastically lower. This shift enabled faster, more efficient image generation with no compromises in quality, hence, Stable Diffusion is one of the most important improvements from its earlier methods, for example GANs~\cite{dhariwal2021diffusionmodelsbeatgans}. By reducing the computational demands for image synthesis, Stable Diffusion has become one of the most used models, though it still requires substantial resources and Rombach et al.\ state, the training of SD 2.1~\cite{rombach2023sd_2_1} took around 200,000 GPU hours alone.
\\
\noindent
A more considerate alternative, when taking environmental and financial costs associated with such highly demanding models into count, is \textbf{Würstchen} by Pernias et al.\ Similar to Stable Diffusion, the backbone of Würstchen are LDMs. However, it makes critical changes with the aim of reducing computational load. Instead of generating images in one step, Würstchen splits the process into three phases, operating mainly in lower-dimensional latent spaces. This drastically reduces training time and computational cost and thus makes the approach more viable, without any sacrifice in image quality.
\\
\noindent
\textbf{ControlNet} brings further enhancements for diffusion-based models. Zhang et al.\ describe a technique for fine-grained control over image generation. While the text prompt may be sufficient for basic image synthesis, generating more complex layouts, poses, or even shapes is hard to achieve with text description only. ControlNet overcomes this limitation by providing conditional control to pre-trained diffusion models like Stable Diffusion. It does so by incorporating a secondary trainable network that handles additional inputs such as edge maps, human poses, or segmentation maps, giving users the ability to specify more detailed and accurate compositions. This flexibility further enhances the usability of diffusion models with more complex tasks, while keeping the robustness of the original model through its architecture, which locks the pre-trained layers while fine-tuning a trainable copy.
\\
\\
\noindent
We provide a deep dive into the technical details of each model, reproduce one experiment from each original research paper, and look into the results of these experiments to understand strengths and weaknesses for each approach. Also, we will consider wider implications for society in general, regarding text-to-image AI, including bias and ethics related to content creation associated with large-scale model training. In doing so, we try to give a picture of the current state of text-to-image AI, as well as future developments leading to a more sustainable and responsible development of the technology.