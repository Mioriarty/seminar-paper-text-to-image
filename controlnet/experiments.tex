\subsection{The Average User Ranking of ControlNet}
\subsubsection{Selection of the Experiment and Background}
We conducted a user study to evaluate the performance of ControlNet compared to PITI~\cite{wang2022pretrainingneedimagetoimagetranslation} in generating images from sketches. We chose to replicate this experiment, because it allows us to get some insight into ControlNet's ability of infering meaning from input conditions only, compared to PITI.
Unlike the original ControlNet study~\cite{zhang2023addingconditionalcontroltexttoimage}, which included Sketch-Guided Diffusion (SGD)~\cite{voynov2022sketchguidedtexttoimagediffusionmodels} and a lighter version of ControlNet (ControlNet-lite), we were limited to PITI and ControlNet because the authors of SGD did not provide a model, and retraining it was not feasible. Additionally, the ControlNet-lite model was unavailable. In the original paper the authors mention both, Stable Diffusion (SD) V1.5 and Stable Diffusion V2 as backbones for ControlNet. However, they do not indicate which version of Stable Diffusion is used for the experiment. As the authors official GitHub~\footnote{https://github.com/lllyasviel/ControlNet} contains the setup instructions for ControlNet + SD V1.5 and we could not find any indication of Stable Diffusion V2, we opted for SD V1.5 for our experiment. Furthermore, unlike PITI, which was trained on the MS-COCO dataset~\cite{lin2015microsoftcococommonobjects} only including photos, ControlNet can also generate images in different styles, i.e.\ cartoons and paintings, due to the fact that SD V1.5 was trained on the LAION-400M dataset~\cite{schuhmann2021laion400mopendatasetclipfiltered}. While the original experiments goal was to evaluate the methods performance on sketch inputs only (wihtout prompts), we also evaluate the difference of ControlNets images generated with simple prompts and with no prompts, because without prompts ControlNet may produce stylized outputs. We use the prompt ``realistic, hd, photo'', as it does not hint at specific objects of the image but implicitly tells ControlNet to generate high quality, non-stylized images. We generate the sketches via the method used by ``Pretraining is All You Need for Image-to-Image
Translation''. The sketches are generated via HED~\cite{xie2015holisticallynestededgedetection}, followed by binarizing the output.

For this study, we sampled 20 unseen sketches and used them to generate images with PITI, ControlNet and ControlNet + prompt. We recruited 17 users to rank each generated image based on two criteria: “the quality of displayed images” and “the fidelity to the sketch,” following a protocol similar to the original study. Participants ranked each of the 60 images on a scale from 1 to 5, where lower scores indicate worse performance, resulting in 60 rankings for each criterion (quality and fidelity) per participant. In the experiment we show the users two images at a time, the sketch and a generated image using the sketch. The order of all pairs is randomized, because users need a few pairs to establish their own baseline. By randomizing the order, we aim to counter the effects of this baseline establishment and other biases. To quantify user preferences, we computed the Average User Ranking (AUR) for both metrics, which served as a comparative measure of how well each model adhered to the sketches and the overall image quality.
\begin{table}[hbp]
    \begin{center}
        \caption{Average User Ranking (AUR) of result quality and
        condition fidelity. The results were calculated by obtaining a user preference ranking (1 to 5 indicates worst to best) for the generated images of PITI, ControlNet and ControlNet with a simple prompt.}
        \label{tab:controlnet:experiment}
        \def\arraystretch{1.1}
        \begin{tabular}{lcc}
            \toprule
            \textbf{Method}     & \textbf{Result Quality} & \textbf{Condition Fidelity}  \\
            \midrule
            PITI~\cite{wang2022pretrainingneedimagetoimagetranslation} (sketch)            & $2.56 \pm 1.20$         & $3.61 \pm 1.13$ \\
            ControlNet          & $3.00 \pm 1.16$         & $3.04 \pm 1.23$ \\
            ControlNet (prompt) & $3.19 \pm 1.20$         & $3.07 \pm 1.22$ \\
            \bottomrule
        \end{tabular}
    \end{center}
\end{table}

% Category: control
%   Average Result Quality: 3.00 ± 1.16
%   Average Condition Fidelity: 3.04 ± 1.23
% Category: piti
%   Average Result Quality: 2.56 ± 1.20
%   Average Condition Fidelity: 3.61 ± 1.13
% Category: prompt
%   Average Result Quality: 3.19 ± 1.20
%   Average Condition Fidelity: 3.07 ± 1.22

\subsubsection{Results}
Table~\ref{tab:controlnet:experiment} presents the Average User Ranking (AUR) results for image generation methods based on two evaluation criteria: \textit{Result Quality} and \textit{Condition Fidelity}. The three methods compared are PITI (sketch), ControlNet, and ControlNet (with prompt), with average user rankings reported alongside standard deviations. The ratings range from 1 (worst) to 5 (best), reflecting the participants evaluation of the generated images.

\paragraph{Result Quality.}
PITI (sketch) achieved the lowest average ranking for result quality, with a score of 2.56 ± 1.20. This indicates that users generally perceived PITI's generated images to be of lower quality compared to the other methods. The relatively high standard deviation of 1.20 suggests significant variability in user preferences, meaning that while some users rated PITI more favorably, others did not. ControlNet achieved a higher average result quality of 3.00 ± 1.16, suggesting it performed better than PITI in producing high-quality images. However, the standard deviation of 1.16 indicates that user ratings still varied considerably. ControlNet (with prompt) obtained the highest result quality score of 3.19 ± 1.20, indicating that adding a simple prompt improved user perceptions of image quality, though not by much. The similar standard deviation across all models implies consistent variability in user preferences across the board.

\paragraph{Condition Fidelity.}
For fidelity to the original sketch, PITI performed relatively well, with an average score of 3.61 ± 1.13. This higher fidelity rating suggests that users felt PITI better adhered to the input sketches compared to the other models. Despite this, the standard deviation of 1.13 suggests some level of disagreement among users. ControlNet received a slightly lower fidelity score of 3.04 ± 1.23, indicating it was less effective at adhering to the original sketch compared to PITI. The larger spread, with a standard deviation of 1.23, reflects more variability in user experiences. ControlNet (with prompt) scored 3.07 ± 1.22, representing a marginal improvement over ControlNet without prompts. This suggests that adding a simple prompt may slightly enhance sketch fidelity, but not enough to surpass PITI. The variability in user ratings remained similar between ControlNet and ControlNet (with prompt).
\\
\\
PITI performed best in terms of \textit{Condition Fidelity} but lagged behind in \textit{Result Quality}. This suggests that while PITI stays true to the input sketches, it sacrifices the overall quality of the output images. ControlNet showed more balanced performance across both criteria, with an improvement in \textit{Result Quality} over PITI but lower \textit{Condition Fidelity}. ControlNet (with prompt) performed best in terms of \textit{Result Quality}, indicating that adding prompts can help boost the perceived image quality, though its impact on \textit{Condition Fidelity} was small. When looking at Table~\ref{tab:controlnet:experiment}, we can see that the standard deviations across all methods and criteria indicate significant variability in user preferences, meaning that while some users rated the methods highly, others did not, suggesting subjective differences in what users considered to be high-quality or similar to the sketch. The different user preferences could be explained by the wide range of participants that performed the experiment. Most participants come from completely different professions and many of them have no major experience with image generating artificial intelligence. This may also significantly impact the final ratings.

When we compare our results to the results of the original paper, we can observe significant differences. While ControlNet ranked higher than PITI, just like in the original experiment, the scores are much closer in our experiment. Zhang et al. report a difference of PITI's and ControlNet's \textit{Result Quality} of 3.12, where our experiment leads to a difference of only 0.44. Eventhough ControlNet still performs better than PITI, user tendencies do not explain this significant change in rating. This leads us to believe that if the ControlNet team used SD V1.5 for their experiment, they may have cherry-picked at least some of the generated images presented to users. However, as previously mentioned, they do not specify the version of Stable Diffusion used. Some examples of the generated images can be seen in Table~\ref{tab:controlnet:example}.
\begin{table}[hbp]
    \centering
    \scalebox{0.76}{
    \begin{tabular}{c c c c c}
        Ground Truth & HED~\cite{xie2015holisticallynestededgedetection} & PITI~\cite{wang2022pretrainingneedimagetoimagetranslation} & ControlNet & ControlNet (prompt) \\
        \includegraphics[width=0.23\textwidth]{./assets/base_image_00066135.pdf} & \includegraphics[width=0.23\textwidth]{./assets/hed_image_00066135.pdf} & \includegraphics[width=0.23\textwidth]{./assets/piti_image_00066135.pdf}  & \includegraphics[width=0.23\textwidth]{./assets/control_image_00066135.pdf} & \includegraphics[width=0.23\textwidth]{./assets/prompt_control_image_00066135.pdf} \\
        \includegraphics[width=0.23\textwidth]{./assets/base_image_00065485.pdf} & \includegraphics[width=0.23\textwidth]{./assets/hed_image_00065485.pdf} & \includegraphics[width=0.23\textwidth]{./assets/piti_image_00065485.pdf} & \includegraphics[width=0.23\textwidth]{./assets/control_image_00065485.pdf} & \includegraphics[width=0.23\textwidth]{./assets/prompt_control_image_00065485.pdf} \\
        \includegraphics[width=0.23\textwidth]{./assets/base_image_00065736.pdf} & \includegraphics[width=0.23\textwidth]{./assets/hed_image_00065736.pdf} & \includegraphics[width=0.23\textwidth]{./assets/piti_image_00065736.pdf} & \includegraphics[width=0.23\textwidth]{./assets/control_image_00065736.pdf} & \includegraphics[width=0.23\textwidth]{./assets/prompt_control_image_00065736.pdf} \\
        \includegraphics[width=0.23\textwidth]{./assets/base_image_00066231.pdf} & \includegraphics[width=0.23\textwidth]{./assets/hed_image_00066231.pdf} & \includegraphics[width=0.23\textwidth]{./assets/piti_image_00066231.pdf} & \includegraphics[width=0.23\textwidth]{./assets/control_image_00066231.pdf} & \includegraphics[width=0.23\textwidth]{./assets/prompt_control_image_00066231.pdf}
    \end{tabular}
    }
    \caption{Examples of generated images, showing the ground truth, the HED image that was used as the input condition, PITI's result, ControlNet's result and lastly, ControlNet's result when using the prompt ``realistic, hd, photo''.}
    \label{tab:controlnet:example}
\end{table}

