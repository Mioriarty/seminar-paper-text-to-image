\subsection{ControlNet}
This subsection builds on related literature~\cite{zhang2023addingconditionalcontroltexttoimage} 
to explain other works contributing to the development of ControlNet. They can be seperated into
the finetuning of neural networks, image diffusion and image-to-image translation.
\\
\noindent
Finetuning neural networks often faces challenges like overfitting, mode collapse, and catastrophic forgetting. Several methods address these, including HyperNetworks, which predict the weights of larger networks and have been adapted for image generation in GANs and Stable Diffusion~\cite{rombach2022stablediffusion}. Adapter methods add layers to pretrained models for task-specific customization, common in NLP~\cite{houlsby2019parameterefficienttransferlearningnlp} and incremental learning in vision tasks~\cite{rosenfeld2018incrementallearningdeepadaptation}. Recent success includes adapters for vision transformers like ViT-Adapter~\cite{chen2023visiontransformeradapterdense} and T2I-Adapter for Stable Diffusion~\cite{mou2023t2iadapterlearningadaptersdig}. Additive learning prevents forgetting by freezing the original model and adding parameters through masks or pruning~\cite{mallya2018piggybackadaptingsinglenetwork, rosenfeld2018incrementallearningdeepadaptation, mallya2018packnetaddingmultipletasks}. Side-Tuning~\cite{zhang2020sidetuningbaselinenetworkadaptation} introduces new functionalities via a side branch by mixing outputs of the original and new networks, while Low-Rank Adaptation (LoRA) circumvents forgetting by learning low-rank parameter offsets~\cite{hu2021loralowrankadaptationlarge}. Zero-initialized layers, as seen in ControlNet, prevent training interference by initializing convolutional layers to zero~\cite{lecun2015deeplearning,rombach2022stablediffusion, karras2018progressivegrowinggansimproved}. Image Diffusion Models have revolutionized image generation~\cite{sohldickstein2015deepunsupervisedlearningusing}, with Latent Diffusion Models (LDM) reducing computation costs by operating in latent space~\cite{rombach2022stablediffusion}. These models, including Stable Diffusion, rely on pretrained language models like CLIP~\cite{radford2021learningtransferablevisualmodels} for state-of-the-art text-to-image generation. Commercial implementations, such as DALL-E2~\cite{2023dalle2} and Midjourney~\cite{2023midjourney}, also use diffusion-based techniques. Controlling these models allows personalized and task-specific image synthesis through prompt engineering, CLIP features, and cross-attention~\cite{parmar2023zeroshotimagetoimagetranslation}. Examples include MakeAScene~\cite{gafni2022makeascenescenebasedtexttoimagegeneration} and SpaText~\cite{Avrahami_2023} for segmentation control, as well as Textual Inversion and DreamBooth for personalized content generation~\cite{gal2022imageworthwordpersonalizing, ruiz2023dreamboothfinetuningtexttoimage}. Image-to-image translation employs conditional GANs~\cite{isola2018imagetoimagetranslationconditionaladversarial, wang2018highresolutionimagesynthesissemantic, zhou2021cocosnetv2fullresolutioncorrespondence, zhu2018multimodalimagetoimagetranslation} and transformers for domain mapping. Methods like the Taming Transformer~\cite{esser2021tamingtransformershighresolutionimage}, Palette, and PITI focus on high-resolution image translation using diffusion models~\cite{saharia2022paletteimagetoimagediffusionmodels, wang2022pretrainingneedimagetoimagetranslation}. Additionally, manipulation of pretrained GANs, particularly StyleGANs, has been widely studied for fine-grained control over image outputs~\cite{richardson2021encodingstylestyleganencoder}.

% OLD VERSION OF RELATED WORK CONTROLNET
% \subsubsection{Finetuning Neural Networks}
% Finetuning neural networks can result in common issues like overfitting, mode collapse, and 
% catastrophic forgetting. Several methodologies go into overcoming these challenges. 
% HyperNetworks initially applied in natural language processing (NLP) train a smaller neural 
% network to predict the weights of another typically larger neural network. More recently, 
% this method has also been applied for image generation with GANs and Stable Diffusion~\cite{rombach2022stablediffusion} to modify output styles~\cite{alaluf2022hyperstylestyleganinversionhypernetworks, dinh2022hyperinverterimprovingstyleganinversion}.
% Adapter methods embed new layers into pretrained models, commonly used in NLP for 
% task-specific customization~\cite{houlsby2019parameterefficienttransferlearningnlp, stickland2019bertpalsprojectedattention}
% and in computer vision for incremental learning~\cite{rosenfeld2018incrementallearningdeepadaptation} 
% and domain adaptation~\cite{rebuffi2018efficientparametrizationmultidomaindeep}. 
% Recent successes include applications in vision transformers~\cite{li2022exploringplainvisiontransformer, li2021benchmarkingdetectiontransferlearning} 
% and ViT-Adapter~\cite{chen2023visiontransformeradapterdense}, 
% as well as the development of T2I-Adapter for adapting Stable Diffusion to external 
% conditions~\cite{mou2023t2iadapterlearningadaptersdig}.
% Additive Learning avoids forgetting by freezing the original model and 
% adding parameters through weight masks~\cite{mallya2018piggybackadaptingsinglenetwork, rosenfeld2018incrementallearningdeepadaptation}, 
% pruning~\cite{mallya2018packnetaddingmultipletasks}, or hard attention~\cite{serr√†2018overcomingcatastrophicforgettinghard}. 
% Side-Tuning~\cite{zhang2020sidetuningbaselinenetworkadaptation} integrates new functionalities 
% via a side branch by mixing outputs of the original and new networks. Low-Rank Adaptation 
% (LoRA)~\cite{hu2021loralowrankadaptationlarge} circumvents forgetting by learning the 
% parameter offsets with low-rank matrices, based on the idea that many over-parameterized 
% models lie in lowdimensional subspaces~\cite{aghajanyan2020intrinsicdimensionalityexplainseffectiveness, li2018measuringintrinsicdimensionobjective}. 
% Zero-initialized layers, as used by ControlNet, are convolution layers initialized to zero, inspired 
% from diffusion models and several other works discussing the initialization and manipulation
% of network weights~\cite{lecun2015deeplearning,rombach2022stablediffusion, karras2018progressivegrowinggansimproved}.

% \subsubsection{Image Diffusion}
% Image Diffusion Models, first proposed by Sohl-Dickstein et al.~\cite{sohldickstein2015deepunsupervisedlearningusing}, 
% have enjoyed extensive success in the image generation task~\cite{dhariwal2021diffusionmodelsbeatgans, kingma2023variationaldiffusionmodels}. 
% Latent Diffusion Models (LDM)~\cite{rombach2022stablediffusion} perform diffusion processes 
% in the latent space which allows for cheaper computations. Text-to-image diffusion models rely 
% on a pre-trained language model like CLIP~\cite{radford2021learningtransferablevisualmodels} 
% to first convert the text inputs to latent vectors to achieve state-of-the-art performance. 
% Notable models include Glide for text-guided image generation and editing~\cite{nichol2022glidephotorealisticimagegeneration}, 
% as well as Stable Diffusion, a large-scale LDM~\cite{rombach2022stablediffusion}. Commercial 
% implementations include DALL-E2~\cite{2023dalle2} and Midjourney~\cite{2023midjourney}. 
% The ability to control image diffusion models enables the creation of personalized, customized
% as well as task-specific image synthesis. This can be done by the manipulation of prompt 
% engineering, CLIP features, and cross-attention among others~\cite{parmar2023zeroshotimagetoimagetranslation, gafni2022makeascenescenebasedtexttoimagegeneration}. 
% Examples include MakeAScene~\cite{gafni2022makeascenescenebasedtexttoimagegeneration} and 
% SpaText~\cite{Avrahami_2023} for segmentation mask control, GLIGEN for 
% learning parameters from an attention layer~\cite{li2023gligenopensetgroundedtexttoimage}, 
% and Textual Inversion and DreamBooth for personalizing content with user-provided images~\cite{gal2022imageworthwordpersonalizing, ruiz2023dreamboothfinetuningtexttoimage}. 
% Alternative approaches are prompt-driven modifications~\cite{brooks2023instructpix2pixlearningfollowimage, huang2023regionawarediffusionzeroshottextdriven, tumanyan2022plugandplaydiffusionfeaturestextdriven} 
% and optimization techniques based on sketches~\cite{voynov2022sketchguidedtexttoimagediffusionmodels}.

% \subsubsection{Image-to-Image Translation}
% Image-to-image translation leverages conditional GANs~\cite{isola2018imagetoimagetranslationconditionaladversarial, wang2018highresolutionimagesynthesissemantic, zhou2021cocosnetv2fullresolutioncorrespondence, zhu2018multimodalimagetoimagetranslation} 
% and transformers~\cite{chen2021pretrainedimageprocessingtransformer,ramesh2021zeroshottexttoimagegeneration} 
% for mapping across image domains. The Taming Transformer represents one such vision 
% transformer method~\cite{esser2021tamingtransformershighresolutionimage}. Palette and PITI are 
% representatives of conditional diffusion models for image-to-image translation; Palette is 
% trained from scratch, whereas PITI relies on pretraining~\cite{saharia2022paletteimagetoimagediffusionmodels, wang2022pretrainingneedimagetoimagetranslation}. 
% Lastly, the manipulation of pretrained GANs, such as StyleGANs controlled by extra encoders, has been 
% extensively studied~\cite{richardson2021encodingstylestyleganencoder}.