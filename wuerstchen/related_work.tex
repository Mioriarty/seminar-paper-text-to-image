\subsection{W\"urstchen}
As latent diffusion models such as Stable
Diffusion~\cite{rombach2022stablediffusion} yield high image quality they come
with two disadvantages: the training time and the computational costs. Thus,
the need for an efficiency improvement arises. In this section, we present other
approaches to solve these issues for general diffusion models.

In 2020, Ho et al.~\cite{ho2020denoisingdiffusionprobabilisticmodels} introduced
the  denoising diffusion probabilistic model (DDPM) which yielded promising
results in generative modeling. Building on this work, Nichol et
al.~\cite{Nichol2021ImprovedDenoisingDiffusionProbabilisticModels} provide a
modification in order to improve training time and sample quality. They
proposed to use cosine-based noise schedules leading to a faster
convergence by distributing noise more effectively during training.
Furthermore, in contrast to DDPM they proposed learnable variances in the
reverse process, allowing the model to better capture uncertainties.

Focussing on the improvement of sampling time and efficiency,
Zhang et al.~\cite{zhang2023fastsamplingdiffusionmodels} suggested the
Diffusion Exponential Integrator Sampler (DEIS) to accelerate the reverse
diffusion process. By employing higher-order solvers for stochastic
differential equations (SDEs), they significantly reduced the number of
sampling steps required, allowing for faster sampling times without
compromising the quality of generated data.

In order to efficiently train large diffusion models, Zheng et
al.~\cite{zheng2024fast} introduce masked transformers. Their approach
selectively masks portions of the input data during training, enabling the
model to focus on reconstructing these masked regions. This selective training
reduces the amount of data processed in each iteration.

W\"urstchen on the other hand, works in compact latent spaces in two-stage
generative process to reduce computational costs while also implementing
cosine-based schedules for the training of the C Stage (see
Section~\ref{sec:wuerstchen:training}) as the improved DDPM does.