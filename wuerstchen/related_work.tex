\subsection{W\"urstchen}
As Stable Diffusion~\cite{rombach2022stablediffusion} models yield high image quality they come with two
disadvantages: the training time and the computational costs. Thus, the need
for a efficacy improvement arises. In this section, we present other approaches
to solve this issue.

In the following, we will present some literature that focuses on improving the
efficiency of general diffusion models. In 2020, Ho et
al.~\cite{ho2020denoisingdiffusionprobabilisticmodels} introduced
the  denoising diffusion probabilistic model (DDPM) which yielded promising
results in generative modeling. Building on this work, Nichol et
al.~\cite{Nichol2021ImprovedDenoisingDiffusionProbabilisticModels} provide a
modification in order to improve training time and sample quality. For
instance, they proposed to use cosine-based noise schedules leading to a faster
convergence by distributing noise more effectively during training.
Furthermore, in contrast to DDPM they introduce learnable variances in the
reverse process, allowing the model to better capture uncertainties.

Zheng et al.~\cite{zhang2023fastsamplingdiffusionmodels} focus on the
improvement of sampling time and efficiency. To achieve this, they propose the
Diffusion Exponential Integrator Sampler (DEIS) which solves stochastic
differential equations more efficiently. W\"urstchen on the other hand, works
in compact latent spaces to reduce computational costs while implementing
cosine-based schedules as the improved DDPM does.

In the context of Stable Diffusion,


