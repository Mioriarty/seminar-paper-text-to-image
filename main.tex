\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[margin=1.25in]{geometry}
\usepackage{subcaption}
\usepackage{cite}
\usepackage{graphicx}
\usepackage[
  plainpages=false,
  pdfpagelayout=TwoPageRight,
  pdfborder={0 0 0},
  hyperfootnotes=false
]{hyperref}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{caption} % For captioning tables and figures
\usepackage{array} % For more flexible table formatting
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{makecell}

\begin{document}

\begin{titlepage}
    \centering
    \includegraphics[width=0.2\textwidth]{assets/uni-logo.png}\par\vspace{0.5cm}
    {\scshape\LARGE Friedrich Schiller University, Jena \par}
    \vspace{1cm}
    {\scshape\Large Seminar Paper for Advanced Methods in Computer Vision\par}
    \vspace{1.5cm}
    {\huge\bfseries An Overview of the Inner Workings, Impacts, and Related Research of Stable Diffusion and Its Offsprings\par}
    \vspace{2cm}
    {\Large\itshape Phillip Rothenbeck, Lean Manthey, and Moritz Seppelt\par}
    \vfill
    supervised by\par
    M.Sc.~Niklas \textsc{Penzel}

    \vfill

    {\large \today\par}
\end{titlepage}

\tableofcontents
\newpage

\include{introduction}

\section{Related Work}
\subsection{Stable Diffusion}
Before Stable Diffusion, generative models for image synthesis faced several challenges due to the high dimensionality of image data. Generative Adversarial Networks (GANs) \cite{goodfellow2014GAN} produced high-resolution images with strong perceptual quality \cite{bigganbrock, DBLP:journals/corr/abs-1912-04958} but were difficult to optimize and struggled to fully capture data distributions \cite{arjovsky2017wasserstein, gulrajani2017improved, DBLP:journals/corr/abs-1801-04406}. Likelihood-based methods such as Variational Autoencoders (VAEs) \cite{VAE} and flow-based models \cite{dinh2015nice, DBLP:conf/iclr/DinhSB17} improved density estimation and allowed efficient high-resolution synthesis \cite{DBLP:journals/corr/abs-2011-10650, glow, DBLP:conf/nips/VahdatK20}, but their sample quality lagged behind GANs. Autoregressive models (ARMs) \cite{DBLP:conf/icml/ChenRC0JLS20, DBLP:journals/corr/abs-1904-10509, NIPS2016_b1301141, DBLP:journals/corr/OordKK16}, while excelling in density estimation, were limited to low-resolution images due to computational complexity and sequential sampling requirements.

Diffusion Probabilistic Models (DMs) \cite{sohldickstein2015deepunsupervisedlearningusing} achieved state-of-the-art results in both density estimation \cite{kingma2023variationaldiffusionmodels} and sample quality \cite{dhariwal2021diffusionmodelsbeatgans}, benefiting from image-like data when paired with neural architectures like UNet \cite{dhariwal2021diffusionmodelsbeatgans, ronneberger2015unetconvolutionalnetworksbiomedical, DBLP:journals/corr/abs-2011-13456}. Despite their success, DMs trained on pixel space suffered from slow inference speeds and high training costs, even with advanced sampling strategies and hierarchical methods \cite{DBLP:journals/corr/abs-2106-15282, DBLP:journals/corr/abs-2106-00132, DBLP:journals/corr/abs-2104-02600, DBLP:conf/iclr/SongME21, DBLP:journals/corr/abs-2106-05931}.

Two-stage approaches \cite{DBLP:conf/iclr/DaiW19, esser2021tamingtransformershighresolutionimage, DBLP:conf/nips/RazaviOV19, DBLP:conf/nips/RombachEO20, DBLP:journals/corr/abs-2104-10157, yu2021vectorquantized} sought to combine the strengths of various generative models. For instance, VQ-VAEs \cite{DBLP:conf/nips/RazaviOV19, DBLP:journals/corr/abs-2104-10157} used ARMs over a discretized latent space, while VQGANs \cite{esser2021tamingtransformershighresolutionimage, yu2021vectorquantized} introduced adversarial and perceptual objectives to scale autoregressive transformers. However, these methods were limited by high computational costs and trade-offs between compression rates and image quality \cite{esser2021tamingtransformershighresolutionimage, ramesh2021zeroshottexttoimagegeneration}.


\input{wuerstchen/related_work}
\input{controlnet/related_work}

\pagebreak
\section{Background}
\subsection{Stable Diffusion}
\label{heading:subsection:stable_diffusion}
In early 2022, computer scientists from LMU Munich and IWR Heidelberg proposed an improved approach to achieve high-resolution image synthesis\cite{rombach2022stablediffusion}. They called this method "latent diffusion models". It introduced the necessary performance boost to make the productive use and training of image models feasible on a large scale. But diffusion models were already known. The new idea was to generate a low-dimensional representation of the image by denoising and then upscaling it, instead of generating the full image by iterative diffusion. This small change was enough to achieve the required performance increase. Later, the descendant family of models was called "Stable Diffusion".

In this chapter, we will give an overview of the inner workings of stable diffusion, focusing on the text-to-image technique. It consists of three main parts:
\begin{enumerate}
    \item Text input is converted into conditioning vectors. This is done using a transformer architecture.
    \item The main step is iterative denoising. Here, the conditioning vectors influence the resulting latent representation. A neural network called U-net does the heavy lifting.
    \item Finally, the latent representation must be converted into the final image. This is done using an auto-decoder.
\end{enumerate}

Let's assume that all machine learning components already have their trained weights and biases. Equipped with these, we will go through step by step how a text input is transformed into an image. After that, the training process is presented.

\subsubsection{Conditioning}
For stable diffusion, the conditioning process is a series of different existing components that are plugged together with the goal of semantically understanding the input prompt. First, the input sentence is tokenized. They used the BERT tokenizer\cite{devlin2019bert}. Basically, tokenization means breaking the input into small pieces called tokens. These tokes can be thought of as words or parts of words.

Second, these tokens are embedded in a Euclidean vector space. This already encodes some semantic meaning. Tokens that have similar meanings are mapped to vectors that point to approximately the same region in this high-dimensional space. For example, the words "achieve" and "accomplish" would be closer than "achieve" and "annoying". But even more, directions or distances can be interpreted. This could be thought of as relations between words. For example, the embedding of "Germany" and "Berlin" is expected to have the same distance as "Spain" and "Madrid".

This simple representation is not enough to understand the input. The meaning of words changes drastically depending on the surrounding words. To quote the English linguist J. R. Firth \textit{You shall know a word by the company it keeps}\cite{firth1962studiesinlinguisticanalysis}. To capture these important relationships, the creators of Stable Diffusion used the so-called cross-attention algorithm\cite{vaswani2023attentionneed}. It adapts the embeddings by selectively attending to other relevant tokens in the sequence, thereby incorporating contextual information. This allows the embeddings to more accurately capture the relationships between words, ensuring that the model's understanding of each token is enriched by its surrounding context.


\iffalse
    It operates by computing three different vectors for each token in the input sequence for each type of relationship there could be in the sentence: the Query ($Q$), the Key ($K$), and the Value ($V$) vectors. These vectors are derived from the embedded token vectors through linear transformations, which are essentially matrix multiplications with learned weight matrices. Specifically, for each token, we calculate:
    $$Q = X \cdot W_Q,\quad K = X\cdot W_K,\quad V = X \cdot W_V$$
    Where $X$ is the matrix of embedded token vectors, and $W_Q$, $W_K$ and $W_V$ are learned weight matrices for the Query, Key, and Value, respectively.

    Once we have the Query ($Q$), Key ($K$), and Value ($V$) vectors for each token, the next step is to determine how much each token should focus on the others. The goal here is to figure out which other words in the sentence are most relevant to the current word, given the context.
    Imagine each Query vector as a question that each word is asking about its context. The Key vectors, on the other hand, represent the possible answers that other words can provide. The dot product between a Query and a Key is like measuring how well the question matches the answer. If the Query and Key vectors are similar, it means that the word being asked about is highly relevant to the current word, and so it should be paid more attention.
    However, to prevent these dot products from getting too large when working with high-dimensional vectors, we scale them down by the square root of the Key vector's dimension, $\sqrt{d_k}$.

    $$\frac{QK^\top}{\sqrt{d_k}}$$

    Next, we convert these scaled dot products into attention scores using a softmax function. The softmax function turns these scores into probabilities, where higher scores mean more attention should be given to that token. For example, if "Harry" is the current word, and its Query vector strongly matches the Key vector of "wizard", the attention mechanism will assign a high probability to "wizard", meaning that "Harry" will focus more on "wizard" in this context.

    $$\mathrm{Attention Weights} = \mathrm{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)$$

    Finally, these attention scores are used to compute a weighted sum of the Value vectors. The Value vectors represent the actual information or content associated with each token. The weighted sum allows the model to combine the most relevant pieces of information from the other words, creating a new, context-aware representation of the current word.

    $$\mathrm{Output} = \sum_i \mathrm{Attention Weights}_i \cdot V_i$$

    This described process is done using many Key, Query and Value matrices to capture a how range of possible influences words can have on others. The output are context aware token embeddings.
\fi

\subsubsection{Iterative Denoising}
Iterative denoising is the crucial step in image generation. Remember, the goal is to generate a latent representation of the final image, which can be thought of as a low-dimensional representation, or just a smaller image. We start with random noise in this reduced size. Typically, this would be just a random sample from a Gaussian distribution. The goal will then be to gradually remove the noise to obtain clear, detailed images that match the description from the prompt.

It starts with a noised image, denoted as $x_T$, where $T$ is the total number of steps in the reverse diffusion process. Then the model uses a neural network which is trained to evaluate the function $\varepsilon_\theta(x_t,t,c)$ that estimates the noise at the current noised image $x_t$, $0 \leq t \leq T$ in each step, $c$ is the conditioning input. By approximating the current noise, we can just remove the noise, to slightly denoise the image.
$$x_{t-1}=x_t-\alpha_t \varepsilon_\theta(x_t,t,c)+ \alpha_t z_t$$
where $\alpha_t$ controls the step size for noise removal, and $\alpha_t z_t$  ($z_t$ is some Gaussian noise) adds controlled noise to maintain some stochasticity. This update moves the noisy image closer to a cleaner version, progressively refining it. This step is performed $T$-times, and at $t=0$ we arrived at a denoised latent representation of the final image.

\paragraph{U-Net}
One might wonder, how we calculate $\varepsilon_\theta(x_t,t,c)$ and thus approximate the current noise. This is done using a neural network architecture called U-Net\cite{ronneberger2015unetconvolutionalnetworksbiomedical}, that is well-suited for tasks where you need to transform one image into another, as is needed here.
The U-Net architecture gets its name from its distinctive "U" shape (see fig. \ref{fig:unet}), which reflects the way the network processes information. The U-Net consists of two main parts: the downsampling path and the upsampling path, connected by a bottleneck in the middle.
In the downsampling path, the U-Net gradually reduces the image size while capturing important features. Once the image reaches the bottleneck, it has been compressed into a highly abstract representation. This is where the most essential features are concentrated, and it serves as the transition point between the downsampling and upsampling paths. The upsampling path then constructs the resulting image, restoring the original size and adding details. Skip connections between corresponding layers in these paths help preserve fine details by directly passing information from the encoder to the decoder.
The result is then the approximated noise at this stage of the denoising process, which can be further used, as described above.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{assets/u-net-illustration-correct-scale2.pdf}
    \caption{Depiction of U-Net architecture (example for 32x32 pixels in the lowest resolution). Image Source \cite{ronneberger2015unetconvolutionalnetworksbiomedical}}
    \label{fig:unet}
\end{figure}

\subsubsection{Decoder}
The last part involves converting the latent representation of an image into the output image. As the latent representation is of lower dimensionality, we need to find a way to add detail. To achieve this, the decoder typically employs a series of upsampling and convolutional layers. These layers progressively increase the spatial dimensions of the latent representation while refining the details at each step. The process is analogous to zooming in on a blurred thumbnail, but with the added capability of restoring fine details based on the encoded information.
Throughout this process, the decoder leverages the rich information captured in the early stages of the model, ensuring that the final image not only fits the structure implied by the latent representation but also accurately reflects the content described by the input prompt. The end result is a high-quality, detailed image that visually matches the semantics of the original text input.


\subsubsection{Training}
\label{sec:stabel_diffusion:training}
In the previous sections, we explored a variety of machine learning components that are integral to achieving text-to-image generation capabilities with models like Stable Diffusion. Specifically, these components include the tokenizer, embeddings, transformer (comprising value, query, and key matrices), the U-Net, and the decoder. Pretrained weights are typically used for the tokenizer, word embeddings, and transformer. However, the U-Net, which predicts the noise in the denoising process, and the decoder, which converts the latent representation into the final image, typically require dedicated training. In this chapter, I will focus on the structure of the training data and the loss functions employed in training these components. Although detailed optimization algorithms and mathematical derivations are beyond the scope of this discussion, a comprehensive understanding begins with the decoder and works backward through the model pipeline.

\paragraph{Training the Decoder}
To effectively train the decoder, we introduce an additional component, the encoder, which is crucial for training but not needed during inference. The encoder maps an input image to its corresponding latent representation, forming a Variational Autoencoder (VAE)\cite{kingma2022autoencodingvariationalbayes} together with the decoder. The VAE is a generative model that learns a compressed representation of data in a latent space, balancing the trade-off between reconstruction fidelity and latent space regularization. The encoder and decoder are trained jointly to minimize a loss function that comprises two key terms:

The reconstruction loss $\mathcal{L}_{\mathrm{reconstruction}}$ measures the fidelity of reconstruction. After encoding an image into its latent representation, the decoder attempts to reconstruct the original image from this representation. The reconstruction loss is typically the mean squared error (MSE) between the original and reconstructed images, encouraging the model to learn latent representations that allow for accurate reconstructions.

To prevent the latent space from becoming arbitrarily dispersed and to encourage smooth transitions between points in the latent space, a regularization term based on the Kullback-Leibler (KL) divergence~\cite{kullback1951OnInformationandSufficiency} is added. The KL divergence measures how much the distribution of latent variables deviates from a predefined prior distribution, often using a standard normal distribution. By minimizing this divergence, the VAE ensures that the latent space is well-structured and that similar images map to nearby points in the latent space. We arrive at the formula:

$$\mathcal{L}_{\mathrm{VAE}} = \mathcal{L}_{\mathrm{reconstruction}} + \beta \mathcal{L}_{\mathrm{KL}}$$
where $\beta$ is a coefficient to control the trade-off between reconstruction fidelity and latent space regularization.

Once trained, the VAE can be used to generate latent representations of images, which are essential for the subsequent training of the U-Net, which we will explore now:

\paragraph{Training the U-Net}
With the trained VAE, the next step is to train the U-Net, a critical component in the denoising process. The U-Net is trained within the framework of a Denoising Diffusion Probabilistic Model (DDPM)\cite{ho2020denoisingdiffusionprobabilisticmodels}, a generative model that iteratively refines a noisy image to recover the underlying clean image. The training process for the U-Net proceeds as follows.

Begin with a labeled dataset of images. In the case of Stable Diffusion, the ImageNet dataset~\cite{deng2009imagenet} was used. Each image is passed through the trained encoder of the VAE to obtain its latent representation. This latent space has lower dimensionality than the original image space, increasing the computational efficiency of the learning process.

Gaussian noise is added to the latent representation of an image over multiple time steps, ultimately transforming it into a distribution of pure noise. This sequence of increasingly noisy images, referred to as Forward Diffusion, constitutes the training data for the U-Net.

The primary task of the U-Net is to predict the noise that was added to a given noisy image. For each time step, the U-Net takes as input the noisy image and a conditioning vector derived from the text label using the pretrained transformer. It then predicts the noise component that was added during the forward diffusion process. The actual noise component is computed as the difference between consecutive noisy images in the sequence.

The training objective is defined as the mean squared error (MSE) between the predicted noise and the actual noise. By minimizing this loss, the U-Net learns to accurately estimate the noise at each step. This enables it to reverse the diffusion process during inference, effectively denoising the image and recovering the clean latent representation.

\include{wuerstchen/background}
\include{controlnet/background}


\newpage
\section{Experiments}
Here our goal was, to test the reproducibility of the selected papers. Since it is computationally not feasible to reproduce every result, we selected one experiment per paper to review. For Stable Diffusion, we investigate the reproducibility and meaningfulness concerning Rombach et al.'s utilized metric. Second, for W\"urstchen, we state the hypothesis that the human perception of the generated images is generally worse than the reported results of Pernias et al.\ and conduct an experiment to proof this hypothesis. Last, to reproduce Zhang et al.'s experiment, we use ControlNet to generate images from sketches and compare it to other methods by performing a user study to obtain an average user ranking per method.

\subsection{The FID of Stable Diffusion}
\subsubsection{Selection of the experiment and background}
\label{sec:experiments:stabel_diffusion:selction_of_fid}
Before looking closer at the kinds of experiments conducted by the Stable Diffusion team, it was clear that we could not reproduce any training-related experiments, as these are generally computationally very expensive. The evaluation metrics appeared more promising, as certain scores were compared with the state-of-the-art models. Most notably, the FID-Score, which was calculated across different datasets.

The FID (Fréchet Inception Distance)\cite{heusel2018ganstrainedtimescaleupdate} score is a metric for evaluating the quality of images generated by generative or text-to-image models. It compares the distribution of generated images with the distribution of real images, providing a measure of how similar the generated images are to the real ones. The FID score calculates the Fréchet distance between two multivariate Gaussian distributions, one representing the features of real images and the other representing the features of generated images. These features are typically extracted using a pre-trained Inception v3 model. If $P_r$ is the distribution of the real images and $P_g$ is the distribution of the generated images, with $\mu_p, \mu_g$ as the respective means and $\Sigma_p, \Sigma_g$ the respective covariance matrices, than the FID score can be calculated as:
$$\mathrm{FID}(P_r, P_g) = \Vert\mu_r - \mu_g\Vert_2^2 + \mathrm{Tr}\left(\Sigma_r + \Sigma_p - 2 (\Sigma_r\Sigma_g)^\frac{1}{2}\right)$$
A lower FID Score indicates that the generated images are more similar to real images, implying higher quality, while a higher FID Score suggests a greater discrepancy between the distributions of real and generated images, indicating lower quality.

Upon analyzing the paper, we noticed that a different rendering of Stable Diffusion was used depending on the dataset. The parameters that were changed mostly included the dimensionality of the latent space and the number of diffusion steps. This seems odd, as it may suggest that the researchers were cherry-picking the best possible results. Additionally, a hypothesis test was omitted, which would have made reproducing these experiments more useful and interesting. This may also indicate that the results were not simply due to chance.

\subsubsection{Experiment description and methodology}
As the text-to-image capabilities of Stable Diffusion were our main focus, we chose to reproduce the "Text-Conditional Image Synthesis" experiment from the original paper. In that experiment, the FID score based on the MS-COCO validation dataset\cite{lin2015microsoftcococommonobjects} was calculated and compared against the result of other state-of-the-art text-to-image models at the time. As described above, a specific version of Stable Diffusion was chosen to achieve this result. In this case, $250$ diffusion steps were used, along with an $8$ times smaller latent space. The paper provided a FID score of $23.31$.

The experiment proceedes as follows. For each image in the dataset, we get its caption and use it to generate an image with Stable Diffusion. This allows us to reproduce the images in the MS-COCO validation dataset. Afterward, we scale the images so that they all have the same dimensions. We selected dimensions of $299\times299$. Finally, we use \texttt{torch-fidelity}\cite{obukhov2020torchfidelity} by PyTorch to calculate the FID score. This process is repeated for the different numbers of diffusion steps.

\subsubsection{Results}
\begin{table}[hbp]
    \centering
    \caption{Comparison of FID score of the paper and other models with our results}
    \label{tab:fid_is_comparison}
    \begin{tabular}{lc}
        \toprule
                                                                               & \textbf{FID} \\
        \midrule
        \textbf{Original Paper}                                                & $23.31$      \\
        \textbf{Our result}                                                    & $24.56$      \\\addlinespace
        CogView \cite{ding2021cogviewmasteringtexttoimagegeneration}           & $27.10$      \\
        LAFITE \cite{zhou2022lafitelanguagefreetrainingtexttoimage}            & $26.94$      \\
        GLIDE \cite{rezende2014stochasticbackpropagationapproximateinference}  & $12.24$      \\
        Make-A-Scene \cite{gafni2022makeascenescenebasedtexttoimagegeneration} & $11.84$      \\
        \bottomrule
    \end{tabular}
\end{table}

As shown in Tab. \ref{tab:fid_is_comparison}, the FID score is slightly worse. However, since error bounds for the FID score were not provided and no hypothesis testing was performed, it is challenging to evaluate this outcome. The metrics for the other image-generating models are the same ones that Stable Diffusion was compared to in the original paper. We can observe that Stable Diffusion's ranking did not change when using our metrics within the provided examples.

Nonetheless, there is a significant difference between the FID scores. This discrepancy may arise from various factors: differences in the calculation pipeline, e.g. by a more modern version of \texttt{torch-fidelity}, randomness in the generated image data or slightly different preprocessing methods.

\subsubsection{Stability of the FID score}
\begin{figure}
    \centering
    % First subfigure: The plot you created
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{assets/num_samples_comparison.pdf}
        \caption{FID scores for datasets in different sizes.}
        \label{fig:num_samples_comparison}
    \end{subfigure}
    \hfill
    % Second subfigure: The PDF file 'num_samples_comparison_corrected.pdf'
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{assets/num_samples_comparison_corrected.pdf}
        \caption{Graph of proposed corrected FID score.}
        \label{fig:num_samples_comparison_corrected}
    \end{subfigure}
    \caption{Comparison of FID scores across different sample sizes. Here, subsets of MS-COCO where used with 250 diffusion steps.}
\end{figure}

In general, we found the TID score to be very unstable. It can only be compared between models when all the calculation and pipeline steps are exactly the same. To illustrate this phenomenon, we calculated the FID for different samples sizes. The MS-COCO validation dataset, has 5000 images. We took subsets of this dataset in various sizes and computed the FID score just as described previously. The results can be seen in Fig. \ref{fig:num_samples_comparison}. As you can see, there is a consistent downwards trend, which indicates that the more samples exist in a given dataset, the better the TID score is. This happens because the FID score measures the difference between two feature distributions. With fewer data points, this difference is more susceptible for outliers, which increases the calculated difference. We would like to highlight that the FID is not only less accurate with fewer data points but simply worse.

As you can see in Fig. \ref{fig:num_samples_comparison}, the downwards trend inherits a predictable pattern. By linear regression of the log-log-plot, we were able to match the function
$$ y(x) = c\cdot \frac{1}{x^p} \qquad\mathrm{with}\quad p = 0.607,\, c = 4208.3$$
onto our data points. Equipped with this knowledge, we are able to propose a corrected FID score, by multiplying the FID score with the number of samples, the score was calculated with, to the $(-p)$-th power. The results can be viewed in Fig. \ref{fig:num_samples_comparison_corrected}. This correction enables us to compare FID scores of different dataset sizes and also yields a more desirable behavior. The FID score is simply less accurate for lower sample sizes, rather than being higher.

There are clear open questions that require further research. $p$ and $c$ might depend on the dataset and/or the image-generating model, rendering the proposed correction useless. On the other hand, if at least $p$ is a global value for the FID score in general, this method would make FID scores of datasets of different sizes comparable.

We were also interested in the FID score when varying the number of diffusion steps. As we did not have the computational power, to generate 5000 images for each number of diffusion steps, we only used a subset of 1780 images of the MS-COCO dataset. We consider the FID calculation with more than 1750 images a sufficiently stable - at least in our case - as beyond that point the corrected FID remains roughly constant (Fig. \ref{fig:num_samples_comparison_corrected}). The results can be seen in \ref{fig:ddms_comparison}.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{assets/ddms_comparison.pdf}
    \caption{FID score by number of diffusion step for a subset of MS-COCO of size 1750.}
    \label{fig:ddms_comparison}
\end{figure}

As shown, the FID score settles at 50 diffusion steps, indicating constant quality from that point onward. In Fig. \ref{fig:ddms_image_comparison}, we compare the generated images for various prompts.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/ddms_image_comparison.pdf}
    \caption{Generated images with different numbers of diffusion steps. On the right, the reference image from the MS-COCO dataset. The prompts where (1) \textit{A woman standing on skiis while posing for the camera.} (2) \textit{Suitcase sitting on the ground with stickers from various countries on it.} (3) \textit{Black and white photo of a scooter carrying numerous bikes.} (4) \textit{A corner of a kitchen with a big fridge.} (5) \textit{A small elephant walks underneath a big elephant.}}
    \label{fig:ddms_image_comparison}
\end{figure}

\include{wuerstchen/experiments}
\include{controlnet/experiments}

\newpage

\section{Societal Impacts}
The capabilities of the mentioned text-to-image models are already extraordinary, but the development did not stop with Würstchen. The quality of image generation has been further improved\cite{lin2023designbenchexploringbenchmarkingdalle} and other capabilities have been added in so-called multimodal models\cite{yasunaga2022retrievalaugmentedmultimodellanguagemodeling}. With this powerful new technology, one might immediately ask where these text-to-image models are being used today, and what side effects their use has on different groups of people.

\subsection{Application in professional contexts}
The most evident impact may be on those who work as professional artists. In 2023, researchers in Korea conducted a survey of 28 professionals working in a variety of artistic fields, from sculpture to video editing\cite{ko2023largescaletexttoimagegenartionmodelsforvisualartists}. 12 of them mentioned that Stable Diffusion and its descendants are useful for generating more reference material, which they mainly use to learn by observing what and how others create and to get inspiration for new ideas. But most of them also said that it wouldn't change their basic working paradigm. Another use case for them was rapid prototyping to enable real-time communication with their clients or artists in different fields. In addition, the artists found that the models had almost no bias in the creation of art, which made it possible to quickly create unconventional images, but also resulted in a lack of ability to personalize the results. They also said that using text-to-image technology limited their creativity because the generated images were influenced only by the text prompt, and text can never fully and accurately describe and image what the artist has in mind. This often made it frustrating to use image-generating AI in their daily work. Despite their shortcomings, it was noted that, text-to-image technologies are supporting professional artists and provide numerous opportunities.

In addition to artists, professionals in other fields are also using Stable Diffusion-like AI. Nowadays, it is easier than ever to create good to great images that you have in mind. The trend can be called "democratization of art". A Finnish study of game industry professionals\cite{vimpari2023texttoimagegenerationaibygameprofessionals} titled "An Adapt-or-die Type of Situation" explores all kinds of different ways this new technology is impacting the industry. Many of the participants were experiencing a fundamental change in their role. A third were using it daily, and another third weekly. Many of the participants, similar to the Korean artists, used AI mainly for prototyping and ideation. In architecture\cite{sekban2022artandarchtitecture}, too, text-to-image models can be used to generate design alternatives, for example, to make a design more energy efficient or sustainable. Education has also emerged as an area where image-generating AI can be applied\cite{vartiainen2023aiincraftseducation}. Again, it is used for ideation, but also as a way for children to externalize their ideas and concepts. Although it was also pointed out that the use of text-image models in the classroom poses several challenges, for example the question of what to assess when grading a student's work, as substantial parts could have been done by the AI. Even in tourism, applications are possible. According to a Chinese study\cite{miao2023aiintourism}, these AI tools have the chance to improve the anticipation, on-site and recollection experience of tourists.

What we have presented is just the peak of many professional applications of Stable Diffusion and its descendants, and we have not yet started with the private use cases. In general, we were able to identify three main applications: final image creation, ideation and prototyping.

\subsection{Risks and Concerns}
Despite all these great opportunities and applications, it is undeniable that there are risks and negative impacts of the boom of this technology.

As it becomes easier than ever to create convincing images, deepfakes are gradually becoming a significant issue. On March 22, 2023, a Twitter post went viral claiming that there had been an explosion at the Pentagon in Washington DC. The post included an AI-generated image. The fake news was allegedly spread by, among others, the Russian broadcaster "Russia Today"\cite{correctiv2023pentagonexplosion}. Posts like this are becoming more realistic and easier to produce. S. Neupane and other researchers from Mississippi State University\cite{neupane2023impactsriskgenerativeai} classify misinformation as one of the five main new attack vectors leveraging artificial intelligence.

A serious concern is the bias that these models impose on the images they generate. In general, image generation models can only reproduce what they have seen in training. This means that underrepresented data will rarely be reproduced. The consequences can be severe. A study by Microsoft\cite{naik2023socialbiasesthroughtexttoimage} showed that 70\% of the people generated by stable diffusion were white. Also, women are either largely underrepresented or overrepresented, most likely in order to avoid this problem. Another notable finding was that when generating images to the prompt "Office in Ethiopia", both DALL-E and Stable Diffusion depict Ethiopia as being in a state of poor economic conditions, while when actually looking for real images, we can clearly deduce that they are closer to offices you might see in Western countries.

Another consequence of the underrepresentation of the Global South is usability issues. In a collaboration of computer scientists from the US, Canada, and Bangladesh\cite{mim2024impactoftexttoimagetoolsinglobalsouth}, a survey of Bangladeshi image practitioners was conducted. It was found that Bangladeshis had problems creating the text prompts. To quote one of their participants \textit{"Text-based prompts need proficiency in English nouns and adjectives to explain the idea of an image to the GAI [Generative AI] system to receive an outcome that matches the image practitioner's imagination."} Another participant complained that certain Bangla words did not have an accurate English translation, in this case she wanted to describe a certain type of rain. In addition, the participants are often poorly educated, so even if text prompts were available in their native language, they wouldn't be able to accurately articulate their perception of the image. Additionally, most of the participants rely on the use of free image creation technology, as traditional, more professional applications such as \textit{Adobe Photoshop} or \textit{Adobe Illustrator} are out of reach or financially unfeasible. The study also noted the impact of the underrepresentation of data from the Global South. They asked Midjourney to depict a future version of the Gawsia market, which resulted in an image where neither the architectural features nor the physical appearance of the people matched those typically seen in Dhaka.

Furthermore, when using generated images in commercial contexts, one question that has not yet been answered is the question of copyright. Who owns a generated image? There are many possible answers to this question: The artists who created the training data, the company that trained the text-to-image model, or the user who created the text prompt. Currently, there are numerous lawsuits on this issue. Most notably, Getty Images (a leading global stock photography and media company) has sued Stability AI (the company behind Stable Diffusion), accusing them of improperly and unlawfully using millions of its copyrighted images to train Stable Diffusion models without permission\cite{cnn2023gettyimagesvsstabilityai}.

\subsection{Summary}

The rapid advancement of text-to-image models, as exemplified by Würstchen and its successors, has had a profound impact across multiple industries. These models have contributed valuable applications to professional fields such as art, architecture, education, and tourism, enabling new ways of ideation, prototyping, and final image creation. Perhaps most importantly for the creative industries, the democratization of art - now more people can easily create high-quality images - is a tectonic shift in the creative industries, empowering professionals and amateurs alike.

These benefits are accompanied by significant risks and challenges, with deepfakes, bias reinforcement in generated content, and underrepresentation of the Global South in training data raising a number of ethical and practical concerns. In addition, the ambiguity of copyright ownership in AI image generation creates legal issues that have yet to be resolved.

It's critical that these issues be addressed in the future to maximize the benefits and minimize the harms as text-to-image technology evolves. Ongoing research, ethical considerations, and regulatory frameworks will shape the positive and negative impacts of this powerful technology on society.

\include{conclusion}

\newpage
\bibliographystyle{unsrt}
\bibliography{references}
\end{document}
